{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 23498,
          "sourceType": "datasetVersion",
          "datasetId": 310
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Credit Card Fraud Let's do some Predictions",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asmaaad37/Machine-Learning/blob/main/Credit_Card_Fraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "organizations_mlg_ulb_creditcardfraud_path = kagglehub.dataset_download('organizations/mlg-ulb/creditcardfraud')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "JlxeZGcVjYrW"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:02.907721Z",
          "iopub.execute_input": "2025-02-15T20:50:02.908066Z",
          "iopub.status.idle": "2025-02-15T20:50:03.268389Z",
          "shell.execute_reply.started": "2025-02-15T20:50:02.908039Z",
          "shell.execute_reply": "2025-02-15T20:50:03.267276Z"
        },
        "id": "65skbmSnjYra"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inroduction to our Dataset**\n",
        "\n",
        "In this notebook, i'll be using various models to check how accurate they are in detecting whether the transaction is a normal payment or a fraud.\n",
        "\n",
        "Let's Start Exploring!!"
      ],
      "metadata": {
        "id": "X3SYjnVpjYrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tasks:**\n",
        "\n",
        "* Divide the Data-frame into 50/50 sub-dataframe ratio each telling \"Fraud\" and \"Non-Fraud\" transactions. (We'll use NearMiss Alog).\n",
        "* Experiment with different classifiers and determine their accuracy.\n",
        "* Also determine the accuracy with a neural net.\n"
      ],
      "metadata": {
        "id": "8RSVm41LjYrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outline**\n",
        "\n",
        "***I. Understanding our data***\n",
        "\n",
        "a) Gather Sense of our data\n",
        "\n",
        "***II. Preprocessing***\n",
        "\n",
        "a) Scaling and Distributing\n",
        "\n",
        "b) Splitting the Data\n",
        "\n",
        "\n",
        "***III. Random UnderSampling and Oversampling***\n",
        "\n",
        "a) Distributing and Correlating\n",
        "\n",
        "b) Anomaly Detection\n",
        "\n",
        "c) Dimensionality Reduction and Clustering (t-SNE)\n",
        "\n",
        "d) Classifiers\n",
        "\n",
        "***What more can be done to this Notebook!***\n",
        "\n",
        "e) A Deeper Look into Logistic Regression\n",
        "\n",
        "f) Oversampling with SMOTE\n",
        "\n",
        "***IV. Testing***\n",
        "\n",
        "a) Testing with Logistic Regression\n",
        "\n",
        "b) Neural Networks Testing (Undersampling vs Oversampling)"
      ],
      "metadata": {
        "id": "tcrK2c7ojYrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What we already know!!**\n",
        "\n",
        "* The transaction amounts are relatively small, with the average transaction amount being approximately USD 88.\n",
        "* There are no missing values in the dataset; therefore, no imputation is required.\n",
        "* The majority of transactions (99.83%) are non-fraudulent, while fraudulent transactions account for only 0.17% of the total dataset."
      ],
      "metadata": {
        "id": "sdXRTsEwjYre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "* https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets/notebook - I learned some really helpful insights by this notebook by: **janiobachmann**"
      ],
      "metadata": {
        "id": "P3P82pC2jYre"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Coding"
      ],
      "metadata": {
        "id": "JqbrIrX6jYre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# dimensionality reduction and visualization\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "import time\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import collections\n",
        "\n",
        "\n",
        "# Other Libraries\n",
        "# Data Splitting & Pipelines\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "# Handling Imbalanced Data\n",
        "from imblearn.over_sampling import SMOTE # Over-sampling\n",
        "from imblearn.under_sampling import NearMiss # Under-sampling\n",
        "# Performance Metrics\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
        "# Class Distribution\n",
        "from collections import Counter\n",
        "# Cross-Validation\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:03.26979Z",
          "iopub.execute_input": "2025-02-15T20:50:03.270317Z",
          "iopub.status.idle": "2025-02-15T20:50:17.610984Z",
          "shell.execute_reply.started": "2025-02-15T20:50:03.27028Z",
          "shell.execute_reply": "2025-02-15T20:50:17.610154Z"
        },
        "id": "ARkKtLW-jYrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting some sense of our data"
      ],
      "metadata": {
        "id": "Rzg3HQUTjYrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('../input/creditcardfraud/creditcard.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:17.612323Z",
          "iopub.execute_input": "2025-02-15T20:50:17.612884Z",
          "iopub.status.idle": "2025-02-15T20:50:21.015337Z",
          "shell.execute_reply.started": "2025-02-15T20:50:17.612857Z",
          "shell.execute_reply": "2025-02-15T20:50:21.014386Z"
        },
        "id": "Pa9P6dhbjYrf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.016551Z",
          "iopub.execute_input": "2025-02-15T20:50:21.016832Z",
          "iopub.status.idle": "2025-02-15T20:50:21.055326Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.016809Z",
          "shell.execute_reply": "2025-02-15T20:50:21.054345Z"
        },
        "id": "xYtHQEsQjYrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check some statistics about our data\n",
        "df.describe()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.056285Z",
          "iopub.execute_input": "2025-02-15T20:50:21.056604Z",
          "iopub.status.idle": "2025-02-15T20:50:21.501011Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.056578Z",
          "shell.execute_reply": "2025-02-15T20:50:21.499958Z"
        },
        "id": "o0dbP-vJjYrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape and info\n",
        "print(\"Shape of out Dataset is: \",df.shape)\n",
        "print()\n",
        "df.info()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.501915Z",
          "iopub.execute_input": "2025-02-15T20:50:21.502188Z",
          "iopub.status.idle": "2025-02-15T20:50:21.552784Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.502162Z",
          "shell.execute_reply": "2025-02-15T20:50:21.55178Z"
        },
        "id": "E6vU-BRpjYrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# We can see above there are not a single null value in our dataframe.\n",
        "# But if there would be, we could have double checked.\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.55374Z",
          "iopub.execute_input": "2025-02-15T20:50:21.55408Z",
          "iopub.status.idle": "2025-02-15T20:50:21.590018Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.554048Z",
          "shell.execute_reply": "2025-02-15T20:50:21.589026Z"
        },
        "id": "b9N1clCDjYrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns in the dataset\n",
        "df.columns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.592884Z",
          "iopub.execute_input": "2025-02-15T20:50:21.593165Z",
          "iopub.status.idle": "2025-02-15T20:50:21.598279Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.593139Z",
          "shell.execute_reply": "2025-02-15T20:50:21.597491Z"
        },
        "id": "W5xCc_9cjYrg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = df['Class'].value_counts(normalize=True) * 100\n",
        "print(f\"No Frauds: {class_counts[0]:.2f}% of the dataset\")\n",
        "print(f\"Frauds: {class_counts[1]:.2f}% of the dataset\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.599976Z",
          "iopub.execute_input": "2025-02-15T20:50:21.60032Z",
          "iopub.status.idle": "2025-02-15T20:50:21.617798Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.600295Z",
          "shell.execute_reply": "2025-02-15T20:50:21.616784Z"
        },
        "id": "IPtiXfLyjYrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms that the dataset is heavily imbalanced,and we can't use this dataset to train our models, if we use this dataframe, we might get a lot of errors and most probably our model will overfit."
      ],
      "metadata": {
        "id": "HySA2YiUjYrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualize our fraud and Non-Fraud classes.\n",
        "class_palette = {0: \"#0101DF\", 1: \"#DF0101\"}\n",
        "\n",
        "sns.countplot(x=\"Class\", data=df, palette=class_palette)\n",
        "\n",
        "plt.title(\"Class Distribution\\n(0: No Fraud | 1: Fraud)\", fontsize=14)\n",
        "\n",
        "plt.xlabel(\"Transcation Classes\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.618793Z",
          "iopub.execute_input": "2025-02-15T20:50:21.619079Z",
          "iopub.status.idle": "2025-02-15T20:50:21.865957Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.619049Z",
          "shell.execute_reply": "2025-02-15T20:50:21.86498Z"
        },
        "id": "fb56HbscjYrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "By seeing the distributions we can have an idea how skewed these features are."
      ],
      "metadata": {
        "id": "CxaSKFAgjYrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(18, 4))\n",
        "\n",
        "columns = [\"Amount\", \"Time\"]\n",
        "colors = [\"red\", \"blue\"]\n",
        "\n",
        "# Creating distribution plot using loop\n",
        "for i, col in enumerate(columns):\n",
        "    sns.distplot(df[col], ax=axes[i], kde=True, color=colors[i])\n",
        "    axes[i].set_title(f\"Distribution of transaction {col}\", fontsize=14)\n",
        "    axes[i].set_xlabel(f\"{col} Value\")\n",
        "    axes[i].set_ylabel(\"Density\")\n",
        "    axes[i].set_xlim(df[col].min(), df[col].max())\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:21.866976Z",
          "iopub.execute_input": "2025-02-15T20:50:21.867329Z",
          "iopub.status.idle": "2025-02-15T20:50:24.372375Z",
          "shell.execute_reply.started": "2025-02-15T20:50:21.867296Z",
          "shell.execute_reply": "2025-02-15T20:50:24.371435Z"
        },
        "id": "PbfW5WTqjYrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SubSampling and Scaling\n",
        "* Scaled amount and scaled time are the columns with scaled values.\n",
        "* There are 492 cases of fraud in our dataset so we can randomly get 492 cases of non-fraud to create our new sub dataframe.\n",
        "* We concat the 492 cases of fraud and non fraud, creating a new sub-sample."
      ],
      "metadata": {
        "id": "nlCUlyU6jYrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why Creating Sub-Samples ?\n",
        "\n",
        "1️⃣ **Overfitting to the Majority Class:**\n",
        "\n",
        "* The model may learn to classify most transactions as non-fraudulent, failing to detect actual fraud cases.\n",
        "* Our goal is to improve fraud detection accuracy rather than just maximizing overall classification accuracy.\n",
        "\n",
        "2️⃣ **Misleading Feature Importance & Correlations:**\n",
        "\n",
        "* Since we do not know the exact meaning of the \"V\" features, we rely on statistical relationships to understand their impact.\n",
        "* If the dataset is imbalanced, these relationships may be skewed, preventing the model from learning meaningful correlations."
      ],
      "metadata": {
        "id": "7E7K53I9jYrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SubSampling and Scaling\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "columns_to_scale = [\"Amount\", \"Time\"]\n",
        "\n",
        "# Robust scaler uses median and IQR, less sensitive to outliers.\n",
        "scaler = RobustScaler()\n",
        "\n",
        "df[[\"Scaled_Amount\", \"Scaled_Time\"]] = scaler.fit_transform(df[columns_to_scale])\n",
        "\n",
        "df.drop(columns=columns_to_scale, inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.373384Z",
          "iopub.execute_input": "2025-02-15T20:50:24.373665Z",
          "iopub.status.idle": "2025-02-15T20:50:24.442899Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.373642Z",
          "shell.execute_reply": "2025-02-15T20:50:24.442034Z"
        },
        "id": "GJq02p56jYrh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Reordering the columns\n",
        "columns_order = [\"Scaled_Amount\", \"Scaled_Time\"] + [col for col in df.columns if col not in [\"Scaled_Amount\", \"Scaled_Time\"]]\n",
        "df = df[columns_order]\n",
        "\n",
        "df.head()\n",
        "# Amount & Time - Scaled now"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.443982Z",
          "iopub.execute_input": "2025-02-15T20:50:24.444319Z",
          "iopub.status.idle": "2025-02-15T20:50:24.498953Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.444284Z",
          "shell.execute_reply": "2025-02-15T20:50:24.498126Z"
        },
        "id": "a5xbJrVHjYri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting the Data (Original DataFrame)**"
      ],
      "metadata": {
        "id": "va6M5aWKjYri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Display Class Distributions\n",
        "class_counts = df[\"Class\"].value_counts(normalize=True) * 100\n",
        "print(f\"No Frauds: {class_counts[0]:.2f}% of the dataset\")\n",
        "print(f\"Frauds: {class_counts[1]:.2f}% of the dataset\")\n",
        "\n",
        "# Define features and target\n",
        "X, y = df.drop(columns=[\"Class\"]), df[\"Class\"]\n",
        "\n",
        "# Stratified k-Fold for balanced train-test split\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_idx, test_idx = next(skf.split(X, y)) # Getting the first split\n",
        "\n",
        "# Create train-test sets\n",
        "original_Xtrain, original_Xtest = X.iloc[train_idx].values, X.iloc[test_idx].values\n",
        "original_ytrain, original_ytest = y.iloc[train_idx].values, y.iloc[test_idx].values\n",
        "\n",
        "# Check label distribution\n",
        "for label, counts in zip([\"Train\", \"Test\"], [original_ytrain, original_ytest]):\n",
        "    unique, count = np.unique(counts, return_counts=True)\n",
        "    print(f'{label} Label Distribution: {count / len(counts)}')\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.499788Z",
          "iopub.execute_input": "2025-02-15T20:50:24.500087Z",
          "iopub.status.idle": "2025-02-15T20:50:24.647229Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.500062Z",
          "shell.execute_reply": "2025-02-15T20:50:24.646402Z"
        },
        "id": "Zn407RPVjYri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Under-Sampling:**"
      ],
      "metadata": {
        "id": "KBK87BxnjYri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Class\"].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.648113Z",
          "iopub.execute_input": "2025-02-15T20:50:24.648363Z",
          "iopub.status.idle": "2025-02-15T20:50:24.656303Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.64834Z",
          "shell.execute_reply": "2025-02-15T20:50:24.65559Z"
        },
        "id": "BVzdLIwpjYri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "# Shuffle data\n",
        "df = shuffle(df, random_state=42)\n",
        "\n",
        "# Separate Fraud and Non-Fraud Classes\n",
        "fraud_df = df[df[\"Class\"] == 1]\n",
        "non_fraud_df = df[df[\"Class\"] == 0].sample(n=len(fraud_df), random_state=42) # Match Fraud Count\n",
        "\n",
        "# Combine and Shuffle\n",
        "new_df = shuffle(pd.concat([fraud_df, non_fraud_df]), random_state=42)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.657299Z",
          "iopub.execute_input": "2025-02-15T20:50:24.657531Z",
          "iopub.status.idle": "2025-02-15T20:50:24.795781Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.657504Z",
          "shell.execute_reply": "2025-02-15T20:50:24.794803Z"
        },
        "id": "9hdBIQGvjYri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.796609Z",
          "iopub.execute_input": "2025-02-15T20:50:24.796932Z",
          "iopub.status.idle": "2025-02-15T20:50:24.818523Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.7969Z",
          "shell.execute_reply": "2025-02-15T20:50:24.817732Z"
        },
        "id": "jMsMBhgMjYri"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "osj2g0qRjYri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Distributions\n",
        "class_distributions = new_df[\"Class\"].value_counts(normalize=True)\n",
        "print(f\"Distribution of the Classes in the subsample dataset:\\n{class_distributions}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.819536Z",
          "iopub.execute_input": "2025-02-15T20:50:24.819858Z",
          "iopub.status.idle": "2025-02-15T20:50:24.833889Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.819826Z",
          "shell.execute_reply": "2025-02-15T20:50:24.833064Z"
        },
        "id": "M2Ld8BUkjYrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Class Distributions\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Class', data=new_df, palette=['#1f77b4', '#ff7f0e'])\n",
        "plt.title(\"Equally Distributed Classes\", fontsize=14)\n",
        "plt.xlabel(\"Class (0: No Fraud) | 1: Fraud\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.834784Z",
          "iopub.execute_input": "2025-02-15T20:50:24.835034Z",
          "iopub.status.idle": "2025-02-15T20:50:24.983606Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.835012Z",
          "shell.execute_reply": "2025-02-15T20:50:24.982726Z"
        },
        "id": "Vqc0KTxHjYrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Correlation Matrices**"
      ],
      "metadata": {
        "id": "ciwuwQIvjYrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up figure with two subplots\n",
        "fig, axes = plt.subplots(nrows=2, figsize=(18, 16))\n",
        "\n",
        "# Correlation matix for imbalanced dataset\n",
        "sns.heatmap(df.corr(), cmap='coolwarm', annot=False, ax=axes[0])\n",
        "axes[0].set_title(\"Imbalanced Correlation Matrix (Not Reliable)\", fontsize=16)\n",
        "\n",
        "# Correlation Matrix for balanced dataset\n",
        "sns.heatmap(new_df.corr(), cmap='coolwarm', annot=False, ax=axes[1])\n",
        "axes[1].set_title(\"Subsample Correlation Matrix (More Reliable)\", fontsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:24.984478Z",
          "iopub.execute_input": "2025-02-15T20:50:24.985016Z",
          "iopub.status.idle": "2025-02-15T20:50:27.227446Z",
          "shell.execute_reply.started": "2025-02-15T20:50:24.984977Z",
          "shell.execute_reply": "2025-02-15T20:50:27.226515Z"
        },
        "id": "YwA2rhFXjYrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BoxPlots for Negative Correlations**"
      ],
      "metadata": {
        "id": "5T27tZoQjYrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define negatively correlated features\n",
        "neg_corr_features = [\"V17\", \"V14\", \"V12\", \"V10\"]\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(ncols=len(neg_corr_features), figsize=(18, 5))\n",
        "\n",
        "# Boxplots\n",
        "for i, feature in enumerate(neg_corr_features):\n",
        "    sns.boxplot(x=\"Class\", y=feature, data=new_df, palette=[\"#0101DF\", \"#DF0101\"], ax=axes[i])\n",
        "    axes[i].set_title(f\"{feature} vs Class Negative Correlation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:27.228482Z",
          "iopub.execute_input": "2025-02-15T20:50:27.228859Z",
          "iopub.status.idle": "2025-02-15T20:50:28.101758Z",
          "shell.execute_reply.started": "2025-02-15T20:50:27.228825Z",
          "shell.execute_reply": "2025-02-15T20:50:28.10077Z"
        },
        "id": "4hsOKbezjYrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define positively correlated features\n",
        "pos_corr_features = [\"V2\", \"V4\", \"V11\", \"V19\"]\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(ncols=len(pos_corr_features), figsize=(18, 5))\n",
        "\n",
        "# Boxplots\n",
        "for i, feature in enumerate(pos_corr_features):\n",
        "    sns.boxplot(x=\"Class\", y=feature, data=new_df, palette=[\"#0101DF\", \"#DF0101\"], ax=axes[i])\n",
        "    axes[i].set_title(f\"{feature} vs Class Positive Correlation\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:28.102769Z",
          "iopub.execute_input": "2025-02-15T20:50:28.103069Z",
          "iopub.status.idle": "2025-02-15T20:50:28.770465Z",
          "shell.execute_reply.started": "2025-02-15T20:50:28.103035Z",
          "shell.execute_reply": "2025-02-15T20:50:28.769446Z"
        },
        "id": "BGNtvG5ejYrj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers Detection: (Only Extreme Ones)"
      ],
      "metadata": {
        "id": "fUehQA3hjYrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# Define features to visualize\n",
        "features = [\"V14\", \"V12\", \"V10\"]\n",
        "colors = [\"#FB8861\", \"#56F9BB\", \"#C5B3F9\"]\n",
        "\n",
        "# Fig and Subplots\n",
        "fig, axes = plt.subplots(1, len(features), figsize=(18, 6))\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    fraud_dist = new_df[feature].loc[new_df[\"Class\"] == 1].values\n",
        "\n",
        "    # Histogram\n",
        "    sns.histplot(fraud_dist, ax=axes[i], kde=True, stat=\"density\", color=colors[i], bins=30)\n",
        "\n",
        "    # Ovelaying a normal distribution curve\n",
        "    xmin, xmax = axes[i].get_xlim()\n",
        "    x = np.linspace(xmin, xmax, 100)\n",
        "    p = norm.pdf(x, np.mean(fraud_dist), np.std(fraud_dist))\n",
        "    axes[i].plot(x, p, 'k', linewidth=2)   # Normal Curve\n",
        "\n",
        "    axes[i].set_title(f\"{feature} Distribution \\n (Fraud Transactions)\", fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:28.774516Z",
          "iopub.execute_input": "2025-02-15T20:50:28.77481Z",
          "iopub.status.idle": "2025-02-15T20:50:29.568663Z",
          "shell.execute_reply.started": "2025-02-15T20:50:28.774787Z",
          "shell.execute_reply": "2025-02-15T20:50:29.567691Z"
        },
        "id": "fCtmqwZYjYro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction and Clustering:\n",
        "\n",
        "* The t-SNE algorithm accurately clusters fraud and non-fraud cases, even with a small subsample.\n",
        "* Shuffling the dataset before applying t-SNE ensures reliable clustering across different scenarios.\n",
        "* The results suggest that predictive models will effectively distinguish fraudulent transactions.\n",
        "\n",
        "**Note**: If you want a simple instructive video look at [StatQuest: t-SNE, Clearly Explained](http://) by Joshua Starmer"
      ],
      "metadata": {
        "id": "P5tbX2WUjYro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Define dimensionality reduction techniques\n",
        "methods = {\n",
        "    \"t-SNE\": TSNE(n_components=2, random_state=42),\n",
        "    \"PCA\": PCA(n_components=2, random_state=42),\n",
        "    \"Truncated SVD\": TruncatedSVD(n_components=2, algorithm=\"randomized\", random_state=42),\n",
        "}\n",
        "\n",
        "X = new_df.drop('Class', axis=1).values\n",
        "y = new_df['Class'].values\n",
        "\n",
        "# Plot setup\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "colors = {0: 'blue', 1: 'red'}  # Non-Fraud = Blue, Fraud = Red\n",
        "\n",
        "for i, (name, model) in enumerate(methods.items()):\n",
        "    t0 = time.time()\n",
        "    X_reduced = model.fit_transform(X)\n",
        "    t1 = time.time()\n",
        "\n",
        "    print(f\"{name} took {t1 - t0:.2f} seconds\")\n",
        "\n",
        "    # Scatter plot of reduced dimensions\n",
        "    scatter = axes[i].scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap=\"coolwarm\", alpha=0.6)\n",
        "    axes[i].set_title(f\"{name} Projection\", fontsize=14)\n",
        "    axes[i].set_xlabel(\"Component 1\")\n",
        "    axes[i].set_ylabel(\"Component 2\")\n",
        "\n",
        "# Add legend\n",
        "plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', label='Non-Fraud', markersize=8, markerfacecolor='blue'),\n",
        "                    plt.Line2D([0], [0], marker='o', color='w', label='Fraud', markersize=8, markerfacecolor='red')],\n",
        "           loc=\"upper right\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:29.570405Z",
          "iopub.execute_input": "2025-02-15T20:50:29.570779Z",
          "iopub.status.idle": "2025-02-15T20:50:35.89146Z",
          "shell.execute_reply.started": "2025-02-15T20:50:29.570744Z",
          "shell.execute_reply": "2025-02-15T20:50:35.890395Z"
        },
        "id": "vPZymkjbjYro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# X = new_df.drop('Class', axis=1)\n",
        "# y = new_df['Class']\n",
        "\n",
        "\n",
        "# # T-SNE Implementation\n",
        "# t0 = time.time()\n",
        "# X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
        "# t1 = time.time()\n",
        "# print(\"T-SNE took {:.2} s\".format(t1 - t0))\n",
        "\n",
        "# # PCA Implementation\n",
        "# t0 = time.time()\n",
        "# X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
        "# t1 = time.time()\n",
        "# print(\"PCA took {:.2} s\".format(t1 - t0))\n",
        "\n",
        "# # TruncatedSVD\n",
        "# t0 = time.time()\n",
        "# X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n",
        "# t1 = time.time()\n",
        "# print(\"Truncated SVD took {:.2} s\".format(t1 - t0))"
      ],
      "metadata": {
        "trusted": true,
        "jupyter": {
          "source_hidden": true
        },
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:35.89248Z",
          "iopub.execute_input": "2025-02-15T20:50:35.892767Z",
          "iopub.status.idle": "2025-02-15T20:50:35.896515Z",
          "shell.execute_reply.started": "2025-02-15T20:50:35.892743Z",
          "shell.execute_reply": "2025-02-15T20:50:35.895409Z"
        },
        "id": "H5PA5byCjYro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Classifiers"
      ],
      "metadata": {
        "id": "ZxaIOZ0XjYro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of Classifiers (UnderSampling):**\n",
        "\n",
        "* **Four classifiers** are trained to identify the most effective model for fraud detection.\n",
        "* **Logistic Regression** outperforms the other classifiers in most cases and is selected for further analysis."
      ],
      "metadata": {
        "id": "D6vVZ3-qjYro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = new_df.drop('Class', axis=1)\n",
        "y = new_df['Class']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "\n",
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(),\n",
        "    \"KNearest\": KNeighborsClassifier(),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm  # For prgress bar\n",
        "\n",
        "# Store results in a dictionary\n",
        "results = []\n",
        "\n",
        "# Iterate over classifiers\n",
        "for key, classifier in tqdm(classifiers.items(), desc=\"Training Models\"):\n",
        "    classifier.fit(X_train, y_train)  # Train model\n",
        "    scores = cross_val_score(classifier, X_train, y_train, cv=5)  # 5-fold CV\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        \"Model\": classifier.__class__.__name__,\n",
        "        \"Mean Accuracy\": round(scores.mean() * 100, 2),\n",
        "        \"Std Deviation\": round(scores.std() * 100, 2)  # Model stability\n",
        "    })\n",
        "\n",
        "# Convert results into DataFrame and sort by accuracy\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"Mean Accuracy\", ascending=False)\n",
        "print(results_df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:35.897628Z",
          "iopub.execute_input": "2025-02-15T20:50:35.897935Z",
          "iopub.status.idle": "2025-02-15T20:50:36.237691Z",
          "shell.execute_reply.started": "2025-02-15T20:50:35.897867Z",
          "shell.execute_reply": "2025-02-15T20:50:36.236902Z"
        },
        "id": "bYMlsb9sjYro"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Optimal Hyperparameters.\n",
        "\n",
        "1. Logistic Regression\n",
        "2. K-Nearest Neighbors (KNN)\n",
        "3. Support Vector Classifier (SVC)\n",
        "4. Decision Tree Classifier\n",
        "\n",
        "**GridSearchCV** systematically tries different parameter values and selects the combination that gives the best model performance based on cross-validation.\n",
        "\n",
        "**GridSearchCV** exhaustively searches through all combinations, which can be computationally expensive.\n",
        "\n",
        "One major disadvantage of using **GridSearchCV** is that it is very computational expensive.\n",
        "\n",
        "So, another alternative of GridSearchCV is **RandomizedSearchCV** which is faster, efficient and also provides near optimal solutions.\n",
        "\n",
        "**RandomizedSearchCV**, which randomly samples parameter combinations and finds near-optimal results in less time.\n",
        "\n",
        "But if computational resources are not a concern, **GridSearchCV** remains the best choice for maximum accuracy."
      ],
      "metadata": {
        "id": "CI5DQvT3jYrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, for these classifiers i'll be using *RandomizedSearchCV*, just because it would save me time and resources."
      ],
      "metadata": {
        "id": "_-6ZBP_rjYrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': np.logspace(-3, 3, 7)}\n",
        "random_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=10, random_state=42)\n",
        "random_log_reg.fit(X_train, y_train)\n",
        "log_reg = random_log_reg.best_estimator_\n",
        "\n",
        "# K-Nearest Neighbors\n",
        "knears_params = {\"n_neighbors\": list(range(2, 11)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
        "random_knears = RandomizedSearchCV(KNeighborsClassifier(), knears_params, n_iter=10, random_state=42)\n",
        "random_knears.fit(X_train, y_train)\n",
        "knears_neighbors = random_knears.best_estimator_\n",
        "\n",
        "# Support Vector Classifier\n",
        "svc_params = {'C': np.linspace(0.1, 2, 10), 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n",
        "random_svc = RandomizedSearchCV(SVC(), svc_params, n_iter=10, random_state=42)\n",
        "random_svc.fit(X_train, y_train)\n",
        "svc = random_svc.best_estimator_\n",
        "\n",
        "# Decision Tree Classifier\n",
        "tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2, 10)),\n",
        "              \"min_samples_leaf\": list(range(1, 10))}\n",
        "random_tree = RandomizedSearchCV(DecisionTreeClassifier(), tree_params, n_iter=10, random_state=42)\n",
        "random_tree.fit(X_train, y_train)\n",
        "tree_clf = random_tree.best_estimator_"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:36.238505Z",
          "iopub.execute_input": "2025-02-15T20:50:36.23882Z",
          "iopub.status.idle": "2025-02-15T20:50:38.580185Z",
          "shell.execute_reply.started": "2025-02-15T20:50:36.238794Z",
          "shell.execute_reply": "2025-02-15T20:50:38.579176Z"
        },
        "id": "NDTHQLrOjYrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "\n",
        "# Stratified K-Fold for better class balance in each fold\n",
        "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# hyperparameter search space for each model\n",
        "param_distributions = {\n",
        "    'log_reg': {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
        "    'knears': {'n_neighbors': range(2, 10), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']},\n",
        "    'svc': {'C': [0.1, 1, 10, 100], 'kernel': ['rbf', 'linear']},\n",
        "    'tree': {'criterion': ['gini', 'entropy'], 'max_depth': range(2, 10), 'min_samples_leaf': range(1, 5)}\n",
        "}\n",
        "\n",
        "# Dictionary of models\n",
        "models = {\n",
        "    'log_reg': LogisticRegression(),\n",
        "    'knears': KNeighborsClassifier(),\n",
        "    'svc': SVC(),\n",
        "    'tree': DecisionTreeClassifier()\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV for each model\n",
        "best_estimators = {}\n",
        "for name, model in models.items():\n",
        "    search = RandomizedSearchCV(model, param_distributions[name], cv=strat_kfold, n_iter=10, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "    search.fit(X_train, y_train)\n",
        "    best_estimators[name] = search.best_estimator_\n",
        "\n",
        "# Evaluate the best models\n",
        "for name, model in best_estimators.items():\n",
        "    score = cross_val_score(model, X_train, y_train, cv=strat_kfold).mean()\n",
        "    print(f'{name.upper()} Best Model Cross Validation Score: {round(score * 100, 2)}%')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:38.581183Z",
          "iopub.execute_input": "2025-02-15T20:50:38.581445Z",
          "iopub.status.idle": "2025-02-15T20:50:44.537024Z",
          "shell.execute_reply.started": "2025-02-15T20:50:38.581421Z",
          "shell.execute_reply": "2025-02-15T20:50:44.535937Z"
        },
        "id": "AhuKQwwqjYrp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we had used *GridSearchCV* the results could have been different!"
      ],
      "metadata": {
        "id": "kg9LcFhMjYrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfitting Case\n",
        "\n",
        "log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
        "print('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
        "print('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "svc_score = cross_val_score(svc, X_train, y_train, cv=5)\n",
        "print('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n",
        "\n",
        "tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
        "print('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:44.538315Z",
          "iopub.execute_input": "2025-02-15T20:50:44.53871Z",
          "iopub.status.idle": "2025-02-15T20:50:44.776517Z",
          "shell.execute_reply.started": "2025-02-15T20:50:44.53867Z",
          "shell.execute_reply": "2025-02-15T20:50:44.775671Z"
        },
        "id": "OhjheRHLjYrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit as sss"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:44.777466Z",
          "iopub.execute_input": "2025-02-15T20:50:44.777735Z",
          "iopub.status.idle": "2025-02-15T20:50:44.781768Z",
          "shell.execute_reply.started": "2025-02-15T20:50:44.777707Z",
          "shell.execute_reply": "2025-02-15T20:50:44.780761Z"
        },
        "id": "tYkQpt-tjYrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# We will undersample during cross validating\n",
        "undersample_X = df.drop('Class', axis=1)\n",
        "undersample_y = df['Class']\n",
        "\n",
        "# Define StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n",
        "    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n",
        "\n",
        "# Convert DataFrames to NumPy arrays\n",
        "undersample_Xtrain = undersample_Xtrain.values\n",
        "undersample_Xtest = undersample_Xtest.values\n",
        "undersample_ytrain = undersample_ytrain.values\n",
        "undersample_ytest = undersample_ytest.values\n",
        "\n",
        "# Lists to store metrics\n",
        "undersample_accuracy = []\n",
        "undersample_precision = []\n",
        "undersample_recall = []\n",
        "undersample_f1 = []\n",
        "undersample_auc = []\n",
        "\n",
        "# # Applying NearMiss\n",
        "# X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\n",
        "# print('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n",
        "\n",
        "# Cross Validating the right way\n",
        "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
        "    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n",
        "    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
        "    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n",
        "\n",
        "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
        "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
        "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:44.782646Z",
          "iopub.execute_input": "2025-02-15T20:50:44.782937Z",
          "iopub.status.idle": "2025-02-15T20:50:49.022221Z",
          "shell.execute_reply.started": "2025-02-15T20:50:44.782913Z",
          "shell.execute_reply": "2025-02-15T20:50:49.021366Z"
        },
        "id": "PEMYbcDEjYrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Print final results\n",
        "print(\"Mean Accuracy:\", np.mean(undersample_accuracy))\n",
        "print(\"Mean Precision:\", np.mean(undersample_precision))\n",
        "print(\"Mean Recall:\", np.mean(undersample_recall))\n",
        "print(\"Mean F1 Score:\", np.mean(undersample_f1))\n",
        "print(\"Mean ROC AUC Score:\", np.mean(undersample_auc))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:49.023088Z",
          "iopub.execute_input": "2025-02-15T20:50:49.023438Z",
          "iopub.status.idle": "2025-02-15T20:50:49.03167Z",
          "shell.execute_reply.started": "2025-02-15T20:50:49.023402Z",
          "shell.execute_reply": "2025-02-15T20:50:49.030662Z"
        },
        "id": "eGBFSdcrjYrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Plot LogisticRegression Learning Curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "     f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n",
        "     if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "     # First Estimator\n",
        "     train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "     train_scores_mean = np.mean(train_scores, axis=1)\n",
        "     train_scores_std = np.std(train_scores, axis=1)\n",
        "     test_scores_mean = np.mean(test_scores, axis=1)\n",
        "     test_scores_std = np.std(test_scores, axis=1)\n",
        "     ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "     ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "     ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "     ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "     ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n",
        "     ax1.set_xlabel('Training size (m)')\n",
        "     ax1.set_ylabel('Score')\n",
        "     ax1.grid(True)\n",
        "     ax1.legend(loc=\"best\")\n",
        "\n",
        "\n",
        "     train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "     train_scores_mean = np.mean(train_scores, axis=1)\n",
        "     train_scores_std = np.std(train_scores, axis=1)\n",
        "     test_scores_mean = np.mean(test_scores, axis=1)\n",
        "     test_scores_std = np.std(test_scores, axis=1)\n",
        "     ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "     ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "     ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "     ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "     ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n",
        "     ax2.set_xlabel('Training size (m)')\n",
        "     ax2.set_ylabel('Score')\n",
        "     ax2.grid(True)\n",
        "     ax2.legend(loc=\"best\")\n",
        "\n",
        "\n",
        "     # Third Estimator\n",
        "     train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "     train_scores_mean = np.mean(train_scores, axis=1)\n",
        "     train_scores_std = np.std(train_scores, axis=1)\n",
        "     test_scores_mean = np.mean(test_scores, axis=1)\n",
        "     test_scores_std = np.std(test_scores, axis=1)\n",
        "     ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "     ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "     ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "     ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "     ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n",
        "     ax3.set_xlabel('Training size (m)')\n",
        "     ax3.set_ylabel('Score')\n",
        "     ax3.grid(True)\n",
        "     ax3.legend(loc=\"best\")\n",
        "\n",
        "      # Fourth Estimator\n",
        "     train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "     train_scores_mean = np.mean(train_scores, axis=1)\n",
        "     train_scores_std = np.std(train_scores, axis=1)\n",
        "     test_scores_mean = np.mean(test_scores, axis=1)\n",
        "     test_scores_std = np.std(test_scores, axis=1)\n",
        "     ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "     ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "     ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "     ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "     ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n",
        "     ax4.set_xlabel('Training size (m)')\n",
        "     ax4.set_ylabel('Score')\n",
        "     ax4.grid(True)\n",
        "     ax4.legend(loc=\"best\")\n",
        "     return plt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:49.032938Z",
          "iopub.execute_input": "2025-02-15T20:50:49.033224Z",
          "iopub.status.idle": "2025-02-15T20:50:49.0867Z",
          "shell.execute_reply.started": "2025-02-15T20:50:49.0332Z",
          "shell.execute_reply": "2025-02-15T20:50:49.085575Z"
        },
        "id": "X1psTr99jYrq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
        "plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4);"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:50:49.087694Z",
          "iopub.execute_input": "2025-02-15T20:50:49.088034Z",
          "iopub.status.idle": "2025-02-15T20:51:02.967241Z",
          "shell.execute_reply.started": "2025-02-15T20:50:49.087996Z",
          "shell.execute_reply": "2025-02-15T20:51:02.966265Z"
        },
        "id": "Ubk-79DRjYrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "# Create a DataFrame with all the scores and the classifiers names.\n",
        "\n",
        "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
        "                             method=\"decision_function\")\n",
        "\n",
        "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
        "\n",
        "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
        "                             method=\"decision_function\")\n",
        "\n",
        "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:52:06.618405Z",
          "iopub.execute_input": "2025-02-15T20:52:06.618796Z",
          "iopub.status.idle": "2025-02-15T20:52:06.83735Z",
          "shell.execute_reply.started": "2025-02-15T20:52:06.618765Z",
          "shell.execute_reply": "2025-02-15T20:52:06.836595Z"
        },
        "id": "DQbU_6GbjYrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ROC_AUC Score**"
      ],
      "metadata": {
        "id": "E04OmY9hjYrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
        "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
        "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
        "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-15T20:52:15.107222Z",
          "iopub.execute_input": "2025-02-15T20:52:15.107535Z",
          "iopub.status.idle": "2025-02-15T20:52:15.12269Z",
          "shell.execute_reply.started": "2025-02-15T20:52:15.107511Z",
          "shell.execute_reply": "2025-02-15T20:52:15.121538Z"
        },
        "id": "8Kt97v98jYrr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "SIPbn1MvjYrr"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}